{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Museums in the Pandemic - BERT models\n",
    "\n",
    "**Author**: Andrea Ballatore (Birkbeck, University of London)\n",
    "\n",
    "**Abstract**: TODO\n",
    "\n",
    "## Setup\n",
    "This is to check that your environment is set up correctly (it should print 'env ok', ignore warnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda env: mip_v1\n",
      "/Users/andreab/Dropbox/DRBX_Docs/Work/Projects/github_projects/museums-in-the-pandemic/mip/notebooks\n",
      "env ok\n"
     ]
    }
   ],
   "source": [
    "# Test geospatial libraries\n",
    "# check environment\n",
    "import os\n",
    "print(\"Conda env:\", os.environ['CONDA_DEFAULT_ENV'])\n",
    "if os.environ['CONDA_DEFAULT_ENV'] != 'mip_v1':\n",
    "    raise Exception(\"Set the environment 'mip_v1' on Anaconda. Current environment: \" + os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "# spatial libraries \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "#import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from `mip` project\n",
    "print(os.getcwd())\n",
    "fpath = os.path.abspath('../')\n",
    "if not fpath in sys.path:\n",
    "    sys.path.insert(0, fpath)\n",
    "\n",
    "print('env ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/text_analytics_tests/ATIS_dataset-master/data/raw_data/ms-cntk-atis\"\n",
    "\n",
    "\n",
    "def load_ds(fname=os.path.join(DATA_DIR,'/atis.train.pkl')):\n",
    "    \"\"\"\n",
    "    https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b\n",
    "    \"\"\"\n",
    "    with open(fname, 'rb') as stream:\n",
    "        ds,dicts = pickle.load(stream)\n",
    "    print('Done  loading: ', fname)\n",
    "    print('      samples: {:4d}'.format(len(ds['query'])))\n",
    "    print('   vocab_size: {:4d}'.format(len(dicts['token_ids'])))\n",
    "    print('   slot count: {:4d}'.format(len(dicts['slot_ids'])))\n",
    "    print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))\n",
    "    return ds,dicts\n",
    "\n",
    "\n",
    "# convert Pickle file to arrays\n",
    "def load_atis(filename, add_start_end_token=False, verbose=True):\n",
    "    train_ds, dicts = load_ds(os.path.join(DATA_DIR,filename)) #verbose\n",
    "    t2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids','intent_ids'])\n",
    "    i2t, i2s, i2in = map(lambda d: {d[k]:k for k in d.keys()}, [t2i,s2i,in2i])\n",
    "    query, slots, intent =  map(train_ds.get, ['query', 'slot_labels', 'intent_labels'])\n",
    "\n",
    "    if add_start_end_token:\n",
    "        i2s[178] = 'BOS'\n",
    "        i2s[179] = 'EOS'\n",
    "        s2i['BOS'] = 178\n",
    "        s2i['EOS'] = 179\n",
    "\n",
    "    input_tensor = []\n",
    "    target_tensor = []\n",
    "    query_data = []\n",
    "    intent_data = []\n",
    "    slot_data = []\n",
    "    to_show = np.random.randint(0, len(query)-1, 5)\n",
    "    for i in range(len(query)):\n",
    "        input_tensor.append(query[i])\n",
    "        slot_text = []\n",
    "        slot_vector = []\n",
    "        for j in range(len(query[i])):\n",
    "            slot_text.append(i2s[slots[i][j]])\n",
    "            slot_vector.append(slots[i][j])\n",
    "        if add_start_end_token:\n",
    "            slot_text[0] = 'BOS'\n",
    "            slot_vector[0] = 178\n",
    "            slot_text[-1] = 'EOS'\n",
    "            slot_vector[-1]= 179\n",
    "        target_tensor.append(slot_vector)\n",
    "        q = ' '.join(map(i2t.get, query[i]))\n",
    "        query_data.append(q.replace('BOS', '').replace('EOS',''))\n",
    "        intent_data.append(i2in[intent[i][0]])\n",
    "        slot = ' '.join(slot_text)\n",
    "        slot_data.append(slot[1:-1])\n",
    "        if i in to_show and verbose:\n",
    "          print('Query text:', q)\n",
    "          print('Query vector: ', query[i])\n",
    "          print('Intent label: ', i2in[intent[i][0]])\n",
    "          print('Slot text: ', slot)\n",
    "          print('Slot vector: ', slot_vector)\n",
    "          print('*'*74)\n",
    "    query_data = np.array(query_data)\n",
    "    intent_data = np.array(intent_data)\n",
    "    slot_data = np.array(slot_data)\n",
    "    intent_data_label = np.array(intent).flatten()\n",
    "    return t2i, s2i, in2i, i2t, i2s, i2in, input_tensor, target_tensor, query_data, \\\n",
    "        intent_data, intent_data_label, slot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  loading:  ../../data/text_analytics_tests/ATIS_dataset-master/data/raw_data/ms-cntk-atis/atis.train.pkl\n",
      "      samples: 4978\n",
      "   vocab_size:  943\n",
      "   slot count:  129\n",
      " intent count:   26\n",
      "Query text: BOS a first class flight on american to san francisco on tuesday in the next week EOS\n",
      "Query vector:  [178 180 425 302 428 654 212 851 739 440 654 874 482 827 621 910 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O B-class_type I-class_type O O B-airline_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name O O B-depart_date.date_relative O O\n",
      "Slot vector:  [128, 128, 18, 92, 128, 128, 2, 128, 78, 125, 128, 26, 128, 128, 25, 128, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS does midwest express serve cleveland EOS\n",
      "Query vector:  [178 376 593 409 756 304 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O B-airline_name O O B-city_name O\n",
      "Slot vector:  [128, 128, 2, 128, 128, 17, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS what flights leave pittsburgh july fifth after 7 pm and arrive in san francisco EOS\n",
      "Query vector:  [178 916 429 537 682 507 421 190 139 689 215 236 482 739 440 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O B-fromloc.city_name B-depart_date.month_name B-depart_date.day_number B-depart_time.time_relative B-depart_time.time I-depart_time.time O O O B-toloc.city_name I-toloc.city_name O\n",
      "Slot vector:  [128, 128, 128, 128, 48, 28, 27, 36, 35, 100, 128, 128, 128, 78, 125, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS what kind of plane is used on the earliest flight from boston to san francisco afternoon EOS\n",
      "Query vector:  [178 916 513 646 685 498 892 654 827 387 428 444 266 851 739 440 191 179]\n",
      "Intent label:  aircraft\n",
      "Slot text:  O O O O O O O O O B-flight_mod O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name B-depart_time.period_of_day O\n",
      "Slot vector:  [128, 128, 128, 128, 128, 128, 128, 128, 128, 42, 128, 128, 48, 128, 78, 125, 33, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS what flights are available from boston to washington dc late the twenty fifth or early the twenty sixth EOS\n",
      "Query vector:  [178 916 429 228 244 444 266 851 905 344 530 827 881 421 662 388 827 881\n",
      " 775 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O O O B-fromloc.city_name O B-toloc.city_name B-toloc.state_code B-depart_time.period_mod O B-depart_date.day_number I-depart_date.day_number B-or B-depart_time.period_mod O B-depart_date.day_number I-depart_date.day_number O\n",
      "Slot vector:  [128, 128, 128, 128, 128, 128, 48, 128, 78, 80, 32, 128, 27, 95, 56, 32, 128, 27, 95, 128]\n",
      "**************************************************************************\n",
      "Done  loading:  ../../data/text_analytics_tests/ATIS_dataset-master/data/raw_data/ms-cntk-atis/atis.test.pkl\n",
      "      samples:  893\n",
      "   vocab_size:  943\n",
      "   slot count:  129\n",
      " intent count:   26\n",
      "Query text: BOS which flights leave april twelfth from indianapolis and arrive in montreal around 10 pm EOS\n",
      "Query vector:  [178 920 429 537 227 878 444 489 215 236 482 604 231  10 689 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O B-depart_date.month_name B-depart_date.day_number O B-fromloc.city_name O O O B-toloc.city_name B-arrive_time.time_relative B-arrive_time.time I-arrive_time.time O\n",
      "Slot vector:  [128, 128, 128, 128, 28, 27, 128, 48, 128, 128, 128, 78, 15, 14, 89, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS what ground transportation is available at denver EOS\n",
      "Query vector:  [178 916 457 866 498 244 240 351 179]\n",
      "Intent label:  ground_service\n",
      "Slot text:  O O O O O O O B-city_name O\n",
      "Slot vector:  [128, 128, 128, 128, 128, 128, 128, 17, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS show flights from orlando to kansas city EOS\n",
      "Query vector:  [178 770 429 444 667 851 511 301 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O\n",
      "Slot vector:  [128, 128, 128, 128, 48, 128, 78, 125, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS get flights from milwaukee to dtw EOS\n",
      "Query vector:  [178 447 429 444 595 851 381 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O B-fromloc.city_name O B-toloc.airport_code O\n",
      "Slot vector:  [128, 128, 128, 128, 48, 128, 76, 128]\n",
      "**************************************************************************\n",
      "Query text: BOS show me all the flights from burbank to milwaukee EOS\n",
      "Query vector:  [178 770 581 207 827 429 444 272 851 595 179]\n",
      "Intent label:  flight\n",
      "Slot text:  O O O O O O O B-fromloc.city_name O B-toloc.city_name O\n",
      "Slot vector:  [128, 128, 128, 128, 128, 128, 128, 48, 128, 78, 128]\n",
      "**************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# load ATIS training dataset\n",
    "t2i_train, s2i_train, in2i_train, i2t_train, i2s_train, i2in_train, \\\n",
    "    input_tensor_train, target_tensor_train, \\\n",
    "    query_data_train, intent_data_train, intent_data_label_train, slot_data_train = load_atis('atis.train.pkl')\n",
    "\n",
    "# load ATIS testing dataset\n",
    "t2i_test, s2i_test, in2i_test, i2t_test, i2s_test, i2in_test, \\\n",
    "    input_tensor_test, target_tensor_test, \\\n",
    "    query_data_test, intent_data_test, intent_data_label_test, slot_data_test = load_atis('atis.test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model()\n",
    "\n",
    "labels = intent_data_label_train\n",
    "print(labels)\n",
    "plt.hist(labels)\n",
    "plt.xlabel('intent')\n",
    "plt.ylabel('nb samples')\n",
    "plt.title('intent distribution')\n",
    "plt.xticks(np.arange(len(np.unique(labels))))\n",
    "plt.savefig('../../tmp/intent_labels.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enable GPU for tensorflow\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#n_gpu = torch.cuda.device_count()\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT resources\n",
    "\n",
    "Resources:\n",
    "\n",
    "- Original BERT from Google: https://github.com/google-research/bert\n",
    "  - Short tutorial to run `run_classifier.py`: https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/\n",
    "- Implementation in Transformers: https://huggingface.co/transformers/model_doc/bert.html\n",
    "\n",
    "\n",
    "- Tokenize sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tutorial #1\n",
    "\n",
    "Conda environment `bert_v1` on Ubuntu (does not work on Mac)\n",
    "\n",
    "- Based on https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tarfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bert_dir = '/home/andreab/mip/bert/'\n",
    "\n",
    "# prep data\n",
    "#data_tg = tarfile.open(bert_dir+'data/yelp_review_polarity_csv.tgz')\n",
    "#data_tg.extractall(bert_dir+'data')\n",
    "#data_tg.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  Unfortunately, the frustration of being Dr. Go...\n",
       "1  2  Been going to Dr. Goldberg for over 10 years. ...\n",
       "2  1  I don't know what Dr. Goldberg was like before...\n",
       "3  1  I'm writing this review to give you a heads up...\n",
       "4  2  All the food is great here. But the best thing..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv(bert_dir+'data/yelp_review_polarity_csv/train.csv', header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  2  Contrary to other reviews, I have zero complai...\n",
       "1  1  Last summer I had an appointment to get new ti...\n",
       "2  2  Friendly staff, same starbucks fair you get an...\n",
       "3  1  The food is good. Unfortunately the service is...\n",
       "4  2  Even when we didn't have a car Filene's Baseme..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(bert_dir+'data/yelp_review_polarity_csv/test.csv', header=None)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[0] = (train_df[0] == 2).astype(int)\n",
    "test_df[0] = (test_df[0] == 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362568</th>\n",
       "      <td>362568</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Avoid like the plague! You know things aren't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437241</th>\n",
       "      <td>437241</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Brio's Happy Hour is getting worse and worse. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344848</th>\n",
       "      <td>344848</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>I know its a burger place but the Half Chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101558</th>\n",
       "      <td>101558</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>I typically despise Home Depot, but when I nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203000</th>\n",
       "      <td>203000</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>No witty comments this time. The Cajun burger ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label alpha                                               text\n",
       "362568  362568      0     a  Avoid like the plague! You know things aren't ...\n",
       "437241  437241      0     a  Brio's Happy Hour is getting worse and worse. ...\n",
       "344848  344848      1     a  I know its a burger place but the Half Chicken...\n",
       "101558  101558      1     a  I typically despise Home Depot, but when I nee...\n",
       "203000  203000      0     a  No witty comments this time. The Cajun burger ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training dataframe according to BERT by adding the required columns\n",
    "df_bert = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label':train_df[0],\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "# Splitting training data file into *train* and *dev*\n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    "\n",
    "df_bert_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  Contrary to other reviews, I have zero complai...\n",
       "1   1  Last summer I had an appointment to get new ti...\n",
       "2   2  Friendly staff, same starbucks fair you get an...\n",
       "3   3  The food is good. Unfortunately the service is...\n",
       "4   4  Even when we didn't have a car Filene's Baseme..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating test dataframe according to BERT\n",
    "df_bert_test = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'text': test_df[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "df_bert_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataframes to .tsv format as required by BERT\n",
    "df_bert_train.to_csv(bert_dir+'data/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv(bert_dir+'data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv(bert_dir+'data/test.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(bert_dir+\"bert_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT on Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_os_command(cmd):\n",
    "    ret = subprocess.check_output(cmd, shell=True)\n",
    "    ret = ret.decode(\"utf-8\").strip()\n",
    "    return ret\n",
    "\n",
    "bert_model = \"cased_L-12_H-768_A-12\"\n",
    "\n",
    "run_bert = \"\"\"cd {bert_dir} && python run_classifier.py \n",
    "--task_name=cola \n",
    "--do_train=true \n",
    "--do_eval=true \n",
    "--do_predict=true \n",
    "--data_dir=./data/ \n",
    "--vocab_file=./{model}/vocab.txt \n",
    "--bert_config_file=./{model}/bert_config.json \n",
    "--init_checkpoint=./{model}/bert_model.ckpt \n",
    "--max_seq_length=128 \n",
    "--train_batch_size=32 \n",
    "--learning_rate=2e-5 \n",
    "--num_train_epochs=3.0 \n",
    "--output_dir=./bert_output/ \n",
    "--do_lower_case=False\"\"\".format(bert_dir=bert_dir, model=bert_model).replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd /home/andreab/mip/bert/ && python run_classifier.py --task_name=cola --do_train=true --do_eval=true --do_predict=true --data_dir=./data/ --vocab_file=./cased_L-12_H-768_A-12/vocab.txt --bert_config_file=./cased_L-12_H-768_A-12/bert_config.json --init_checkpoint=./cased_L-12_H-768_A-12/bert_model.ckpt --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=./bert_output/ --do_lower_case=False\n"
     ]
    }
   ],
   "source": [
    "print(run_bert)\n",
    "#ret = run_os_command(run_bert)\n",
    "#ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tutorial #2\n",
    "\n",
    "- ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]  i want to fly from boston at 838 am and arrive in denver at 1110 in the morning  [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 681158.73B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'i', 'want', 'to', 'fly', 'from', 'boston', 'at', '83', '##8', 'am', 'and', 'arrive', 'in', 'denver', 'at', '111', '##0', 'in', 'the', 'morning', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# add special tokens for BERT to work properly\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in query_data_train]\n",
    "print(sentences[0])\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT expects input data in a specific format, with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). Furthermore, we need to tokenize our text into tokens that correspond to BERT’s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1045, 2215, ...,    0,    0,    0],\n",
       "       [ 101, 2054, 7599, ...,    0,    0,    0],\n",
       "       [ 101, 2054, 2003, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2029, 7608, ...,    0,    0,    0],\n",
       "       [ 101, 2515, 6803, ...,    0,    0,    0],\n",
       "       [ 101, 2003, 2045, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the maximum sequence length. \n",
    "MAX_LEN = 128\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT’s clever language modeling task masks 15% of words in the input and asks the model to predict the missing word. To make BERT better at handling relationships between multiple sentences, the pre-training process also included an additional task: given two sentences (A and B), is B likely to be the sentence that follows A? Therefore we need to tell BERT what task we are solving by using the concept of attention mask and segment mask. In our case, all words in a query will be predicted and we do not have multiple sentences per query. We define the mask below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4978"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "    \n",
    "len(attention_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create all tensors and iterators needed during fine-tuning of BERT using our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "                                             \n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Select a batch size for training. \n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is time to fine-tune the BERT model so that it outputs the intent class given a user query string. For this purpose, we use the BertForSequenceClassification, which is the normal BERT model with an added single linear layer on top for classification. Below we display a summary of the model. The encoder summary is shown only once. The same summary would normally be repeated 12 times. We display only 1 of them for simplicity sake. We can see the BertEmbedding layer at the beginning, followed by a Transformer architecture for each encoder layer: BertAttention, BertIntermediate, BertOutput. At the end, we have the Classifier layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bb665807e284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=nb_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-eb8ea8f7feff>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-eb8ea8f7feff>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    (bert): BertModel(\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# BERT model summary\n",
    "BertForSequenceClassification(\n",
    "  (bert): BertModel(\n",
    "    (embeddings): BertEmbeddings(\n",
    "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (token_type_embeddings): Embedding(2, 768)\n",
    "      (LayerNorm): BertLayerNorm()\n",
    "      (dropout): Dropout(p=0.1)\n",
    "    )\n",
    "    (encoder): BertEncoder(\n",
    "      (layer): ModuleList(\n",
    "        (0): BertLayer(\n",
    "          (attention): BertAttention(\n",
    "            (self): BertSelfAttention(\n",
    "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (dropout): Dropout(p=0.1)\n",
    "            )\n",
    "            (output): BertSelfOutput(\n",
    "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "              (LayerNorm): BertLayerNorm()\n",
    "              (dropout): Dropout(p=0.1)\n",
    "            )\n",
    "          )\n",
    "          (intermediate): BertIntermediate(\n",
    "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "          )\n",
    "          (output): BertOutput(\n",
    "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        '\n",
    "        '\n",
    "        '\n",
    "      )\n",
    "    )\n",
    "    (pooler): BertPooler(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (activation): Tanh()\n",
    "    )\n",
    "  )\n",
    "  (dropout): Dropout(p=0.1)\n",
    "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. Training the classifier is relatively inexpensive. The bottom layers have already great English words representation, and we only really need to train the top layer, with a bit of tweaking going on in the lower levels to accommodate our task. This is a variant of transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-4bca0d87728d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BERT fine-tuning parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gamma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer_grouped_parameters = [\n\u001b[1;32m      5\u001b[0m     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "  \n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "# Number of training epochs \n",
    "epochs = 4\n",
    "\n",
    "# BERT training loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "  \n",
    "  ## TRAINING\n",
    "  \n",
    "  # Set our model to training mode\n",
    "  model.train()  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "       \n",
    "  ## VALIDATION\n",
    "\n",
    "  # Put model in evaluation mode\n",
    "  model.eval()\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "# plot training performance\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is the moment of truth. Is BERT overfitting? Or is it doing better than our previous LSTM network? We now load the test dataset and prepare inputs just as we did with the training set. We then create tensors and run the model on the dataset in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in query_data_test]\n",
    "labels = intent_data_label_test\n",
    "\n",
    "# tokenize test data\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "MAX_LEN = 128\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "# create test tensors\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "batch_size = 32  \n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "## Prediction on test set\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "  \n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "for i in range(len(true_labels)):\n",
    "  matthews = matthews_corrcoef(true_labels[i],\n",
    "                 np.argmax(predictions[i], axis=1).flatten())\n",
    "  matthews_set.append(matthews)\n",
    "  \n",
    "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With BERT we are able to get a good score (95.93%) on the intent classification task. This demonstrates that with a pre-trained BERT model it is possible to quickly and effectively create a high-quality model with minimal effort and training time using the PyTorch interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for multilabel classification\n",
    "\n",
    "- https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1\n",
    "\n",
    "\n",
    "- Short example: https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "\n",
    "- very short example with Basic BERT: \n",
    "https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*sqwdLJj2HlRPkkOmyrKbtw.png\" />\n",
    "\n",
    "- input_ids: list of numerical ids for the tokenised text\n",
    "- input_mask: will be set to 1 for real tokens and 0 for the padding tokens\n",
    "- segment_ids: for our case, this will be set to the list of ones\n",
    "- label_ids: one-hot encoded labels for the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "        \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PreTrainedBertModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4124180b9485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from pytorch_pretrained_bert.modeling import BertPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertForMultiLabelSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreTrainedBertModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \"\"\"BERT model for classification.\n\u001b[1;32m      5\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcomposed\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mBERT\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtop\u001b[0m \u001b[0mof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PreTrainedBertModel' is not defined"
     ]
    }
   ],
   "source": [
    "#from pytorch_pretrained_bert.modeling import BertPreTrainedModel\n",
    "\n",
    "class BertForMultiLabelSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
