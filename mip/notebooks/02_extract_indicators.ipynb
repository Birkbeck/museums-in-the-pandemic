{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Museums in the Pandemic - Extract indicators\n",
    "\n",
    "**Authors**: Andrea Ballatore (KCL)\n",
    "\n",
    "**Abstract**: Extract indicators from museum text.\n",
    "\n",
    "## Setup\n",
    "This is to check that your environment is set up correctly (it should print 'env ok', ignore warnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda env: mip_v1\n",
      "/Users/andreaballatore/Dropbox/DRBX_Docs/Work/Projects/github_projects/museums-in-the-pandemic/mip/notebooks\n",
      "env ok\n"
     ]
    }
   ],
   "source": [
    "# Test geospatial libraries\n",
    "# check environment\n",
    "import os\n",
    "print(\"Conda env:\", os.environ['CONDA_DEFAULT_ENV'])\n",
    "if os.environ['CONDA_DEFAULT_ENV'] != 'mip_v1':\n",
    "    raise Exception(\"Set the environment 'mip_v1' on Anaconda. Current environment: \" + os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "# spatial libraries \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "from termcolor import colored\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "#import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "#import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import from `mip` project\n",
    "print(os.getcwd())\n",
    "fpath = os.path.abspath('../')\n",
    "if not fpath in sys.path:\n",
    "    sys.path.insert(0, fpath)\n",
    "\n",
    "out_folder = '../../'\n",
    "\n",
    "from museums import *\n",
    "from utils import _is_number\n",
    "\n",
    "print('env ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to DB\n",
    "\n",
    "It needs the DCS VPN active to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB connected\n"
     ]
    }
   ],
   "source": [
    "# open connection to DB\n",
    "from db.db import connect_to_postgresql_db\n",
    "\n",
    "db_conn = connect_to_postgresql_db()\n",
    "print(\"DB connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract matches for all museums\n",
    "\n",
    "Using the best deep learning model defined above, find indicators for all museums (from websites and social media)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load deep learning validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 21:13:07.004195: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7ff7858b0fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def prep_match_data(df):\n",
    "    num_df = valid_ann_df.select_dtypes(include=[np.number])\n",
    "    scaler = MinMaxScaler()\n",
    "    # fit and transform in one step\n",
    "    cols = num_df.columns\n",
    "    x_data = pd.DataFrame(scaler.fit_transform(num_df),columns=cols)\n",
    "    return x_data\n",
    "\n",
    "def convert_pred_to_bool(vals):\n",
    "    pred_y = (vals > 0.5).astype(\"bool\")\n",
    "    # unpack results\n",
    "    bool_vals = [item for sublist in pred_y for item in sublist]\n",
    "    return bool_vals\n",
    "\n",
    "valid_ann_df_fn = 'matches_valid_ann_df_v2.pik'\n",
    "valid_ann_df = pd.read_pickle(out_folder+'data/annotations/'+valid_ann_df_fn)\n",
    "\n",
    "valid_match_cnn_model = load_model(out_folder+\"data/analysis/matching_validation/matching_validation_deep_learning_model.h5\")\n",
    "valid_match_cnn_model\n",
    "\n",
    "#x_data = prep_match_data(valid_ann_df)\n",
    "#pred_valid = convert_pred_to_bool(valid_match_cnn_model.predict(x_data))\n",
    "\n",
    "#valid_ann_df['predicted_valid'] = pred_valid\n",
    "\n",
    "#valid_ann_df.to_excel(out_folder+\"tmp/check_deeplearning.xlsx\",index=False)\n",
    "#valid_ann_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all matches from DB for all museums\n",
    "\n",
    "- Dump all matches from DB, after running `an_text` on ALL museums for a given crawling session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_all_matches_from_db 20210304\n",
      "rows 7167891\n"
     ]
    }
   ],
   "source": [
    "# load from DB\n",
    "def get_all_matches_from_db(session_id):\n",
    "    print(\"get_all_matches_from_db\", session_id)\n",
    "    sql = \"\"\"select muse_id, example_id, sentence_id, indicator_code, token_n, lemma_n, \n",
    "                    ann_overlap_lemma, ann_overlap_token, example_len, ann_ex_tokens, page_tokens,\n",
    "                    txt_overlap_lemma, txt_overlap_token, ann_overlap_criticwords\n",
    "        from analytics.text_indic_ann_matches_{} t where keep_stopwords;\"\"\".format(session_id)\n",
    "    df = pd.read_sql(sql, db_conn)\n",
    "    print(\"rows\",len(df))\n",
    "\n",
    "    # write df to file\n",
    "    matches_fn = out_folder+'tmp/matches_dump_df_{}.pik'.format(session_id)\n",
    "    df.to_pickle(matches_fn)\n",
    "    del df\n",
    "    print('\\tsaved',fn)\n",
    "    return matches_fn\n",
    "\n",
    "sessions = ['20210304','20210404']\n",
    "for session_id in sessions:\n",
    "    get_all_matches_from_db(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Predict all matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TODO\n",
    "#x_data = prep_match_data(valid_ann_df)\n",
    "#pred_valid = convert_pred_to_bool(valid_match_cnn_model.predict(x_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### extract stable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
